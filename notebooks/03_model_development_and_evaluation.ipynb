{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a67fb0",
   "metadata": {},
   "source": [
    "# Healthcare No-Show Prediction - Model Development & Evaluation\n",
    "\n",
    "**Author:** Ronit Saxena  \n",
    "**Purpose:** Advanced ML model development and evaluation for healthcare appointment no-show prediction  \n",
    "**Focus:** Model training, evaluation, and interpretability analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "This notebook focuses on model development and evaluation, building upon the data processing and feature engineering from previous notebooks:\n",
    "1. **Model Architecture** - Advanced ensemble methods with hyperparameter optimization\n",
    "2. **Performance Analysis** - Comprehensive evaluation metrics and cross-validation\n",
    "3. **Model Interpretability** - SHAP analysis and feature importance visualization\n",
    "4. **Decision Thresholds** - ROC curve analysis and optimal threshold selection\n",
    "5. **Model Persistence** - Production-ready model serialization\n",
    "\n",
    "*Prerequisites: Run notebooks 01_data_processing_pipeline.ipynb and 02_advanced_feature_engineering.ipynb first*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19c6fb",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Model Dependencies\n",
    "\n",
    "Setting up specialized libraries for model development, evaluation, and interpretation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f9287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model development environment ready\n",
      "\n",
      "Libraries loaded:\n",
      "   â€¢ scikit-learn: 1.7.0\n",
      "   â€¢ shap: 0.48.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optuna' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   â€¢ scikit-learn: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msklearn.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   â€¢ shap: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshap.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   â€¢ optuna: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moptuna\u001b[49m.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'optuna' is not defined"
     ]
    }
   ],
   "source": [
    "# Core ML Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "\n",
    "# Model Development\n",
    "import sklearn\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier,\n",
    "    StackingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_validate,\n",
    "    StratifiedKFold\n",
    ")\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Advanced Analysis\n",
    "import shap\n",
    "import optuna\n",
    "from optuna import create_study\n",
    "import joblib\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"  Model development environment ready\")\n",
    "print(\"\\nLibraries loaded:\")\n",
    "print(f\"   â€¢ scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"   â€¢ shap: {shap.__version__}\")\n",
    "try:\n",
    "    print(f\"   â€¢ optuna: {optuna.__version__}\")\n",
    "except AttributeError:\n",
    "    print(f\"   â€¢ optuna: {optuna.version.__version__}\")\n",
    "except:\n",
    "    print(\"   â€¢ optuna: version unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38a9897",
   "metadata": {},
   "source": [
    "## 2. Model Development Setup\n",
    "\n",
    "Loading the preprocessed data from previous notebooks and preparing for model development:\n",
    "- Loading feature-engineered dataset\n",
    "- Defining model evaluation metrics\n",
    "- Setting up cross-validation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ba479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "preprocessed_data = pd.read_csv('../data/processed_noshow_data_anonymized.csv')\n",
    "print(f\"âœ… Loaded preprocessed data: {preprocessed_data.shape[0]:,} records\")\n",
    "\n",
    "def prepare_modeling_data(df: pd.DataFrame, target_col: str = 'NoShow') -> Tuple:\n",
    "    \"\"\"\n",
    "    Prepare data for modeling, excluding non-feature columns.\n",
    "    \n",
    "    Args:\n",
    "        df: Preprocessed DataFrame with engineered features\n",
    "        target_col: Name of the target variable\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    # Exclude non-feature columns\n",
    "    exclude_cols = [\n",
    "        target_col, 'AppointmentId', 'PatientId',\n",
    "        'AppointmentDate', 'Booked_Date_Time'\n",
    "    ]\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    feature_cols = feature_cols.difference(pd.Index(exclude_cols))\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[feature_cols],\n",
    "        df[target_col],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df[target_col]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸ“Š Modeling Dataset:\")\n",
    "    print(f\"   â€¢ Features: {len(feature_cols)}\")\n",
    "    print(f\"   â€¢ Training samples: {X_train.shape[0]:,}\")\n",
    "    print(f\"   â€¢ Testing samples: {X_test.shape[0]:,}\")\n",
    "    print(f\"   â€¢ No-show rate: {y_train.mean():.2%}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Prepare modeling data\n",
    "X_train, X_test, y_train, y_test = prepare_modeling_data(preprocessed_data)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate_model(model, X: pd.DataFrame, y: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate model performance with multiple metrics.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained sklearn model\n",
    "        X: Feature matrix\n",
    "        y: Target vector\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of performance metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': (y_pred == y).mean(),\n",
    "        'roc_auc': roc_auc_score(y, y_prob),\n",
    "        'precision_0': classification_report(y, y_pred, output_dict=True)['0']['precision'],\n",
    "        'recall_0': classification_report(y, y_pred, output_dict=True)['0']['recall'],\n",
    "        'precision_1': classification_report(y, y_pred, output_dict=True)['1']['precision'],\n",
    "        'recall_1': classification_report(y, y_pred, output_dict=True)['1']['recall']\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"\\nâœ… Model development setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afbab3",
   "metadata": {},
   "source": [
    "## 3. Advanced Model Development\n",
    "\n",
    "Implementing advanced modeling techniques:\n",
    "- Ensemble model architecture with stacking\n",
    "- Bayesian hyperparameter optimization\n",
    "- Cross-validated performance analysis\n",
    "- Learning curve analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedModelDevelopment:\n",
    "    \"\"\"Advanced model development with ensemble stacking and optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_models = None\n",
    "        self.final_model = None\n",
    "        self.feature_importance = None\n",
    "    \n",
    "    def create_stacking_ensemble(self, params: Dict) -> StackingClassifier:\n",
    "        \"\"\"\n",
    "        Create stacking ensemble with optimized base models.\n",
    "        \"\"\"\n",
    "        # Define base models\n",
    "        base_models = [\n",
    "            ('rf', RandomForestClassifier(\n",
    "                n_estimators=params['rf_n_estimators'],\n",
    "                max_depth=params['rf_max_depth'],\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            )),\n",
    "            ('gb', GradientBoostingClassifier(\n",
    "                n_estimators=params['gb_n_estimators'],\n",
    "                max_depth=params['gb_max_depth'],\n",
    "                random_state=42\n",
    "            ))\n",
    "        ]\n",
    "        \n",
    "        # Create stacking model\n",
    "        stacking_model = StackingClassifier(\n",
    "            estimators=base_models,\n",
    "            final_estimator=LogisticRegression(),\n",
    "            cv=3,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        return stacking_model\n",
    "    \n",
    "    def optimize_ensemble(self, X_train: pd.DataFrame, y_train: pd.Series, \n",
    "                        n_trials: int = 50) -> Dict:\n",
    "        \"\"\"\n",
    "        Optimize ensemble hyperparameters using Optuna.\n",
    "        \"\"\"\n",
    "        print(\"ðŸŽ¯ Optimizing ensemble model...\")\n",
    "        \n",
    "        def objective(trial):\n",
    "            # Define hyperparameter space\n",
    "            params = {\n",
    "                'rf_n_estimators': trial.suggest_int('rf_n_estimators', 100, 500),\n",
    "                'rf_max_depth': trial.suggest_int('rf_max_depth', 3, 15),\n",
    "                'gb_n_estimators': trial.suggest_int('gb_n_estimators', 100, 500),\n",
    "                'gb_max_depth': trial.suggest_int('gb_max_depth', 3, 10)\n",
    "            }\n",
    "            \n",
    "            # Create and evaluate model\n",
    "            model = self.create_stacking_ensemble(params)\n",
    "            \n",
    "            # Cross-validation score\n",
    "            scores = cross_validate(\n",
    "                model, X_train, y_train,\n",
    "                cv=cv,\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            return scores['test_score'].mean()\n",
    "        \n",
    "        # Run optimization\n",
    "        study = create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        print(f\"\\nâœ¨ Best CV Score: {study.best_value:.4f}\")\n",
    "        print(\"\\nOptimal Parameters:\")\n",
    "        for param, value in study.best_params.items():\n",
    "            print(f\"   â€¢ {param}: {value}\")\n",
    "            \n",
    "        return study.best_params\n",
    "    \n",
    "    def train_final_model(self, X_train: pd.DataFrame, y_train: pd.Series,\n",
    "                         best_params: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Train final stacking ensemble with optimal parameters.\n",
    "        \"\"\"\n",
    "        print(\"\\nðŸš€ Training final ensemble model...\")\n",
    "        \n",
    "        self.final_model = self.create_stacking_ensemble(best_params)\n",
    "        self.final_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Store base model feature importances\n",
    "        rf_model = self.final_model.named_estimators_['rf']\n",
    "        self.feature_importance = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': rf_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"âœ… Model training complete\")\n",
    "\n",
    "# Initialize and train model\n",
    "model_dev = AdvancedModelDevelopment()\n",
    "\n",
    "# Optimize model\n",
    "best_params = model_dev.optimize_ensemble(X_train, y_train, n_trials=50)\n",
    "\n",
    "# Train final model\n",
    "model_dev.train_final_model(X_train, y_train, best_params)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\n\udcca Model Performance\")\n",
    "print(\"=\" * 50)\n",
    "train_metrics = evaluate_model(model_dev.final_model, X_train, y_train)\n",
    "test_metrics = evaluate_model(model_dev.final_model, X_test, y_test)\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"   â€¢ {metric}: {value:.4f}\")\n",
    "    \n",
    "print(\"\\nTest Metrics:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"   â€¢ {metric}: {value:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    data=model_dev.feature_importance.head(15),\n",
    "    x='importance',\n",
    "    y='feature'\n",
    ")\n",
    "plt.title('Top 15 Most Important Features')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db75df1f",
   "metadata": {},
   "source": [
    "## 4. Model Interpretation & Insights\n",
    "\n",
    "Detailed analysis of model behavior and predictions:\n",
    "- SHAP value analysis for feature impact\n",
    "- Partial dependence plots for key features\n",
    "- Prediction confidence analysis\n",
    "- Error analysis and edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInterpretation:\n",
    "    \"\"\"Advanced model interpretation and analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, X: pd.DataFrame, y: pd.Series):\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.explainer = None\n",
    "        self.shap_values = None\n",
    "        \n",
    "    def generate_shap_values(self, sample_size: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Generate SHAP values for model interpretation.\n",
    "        \"\"\"\n",
    "        print(\"ðŸ” Calculating SHAP values...\")\n",
    "        \n",
    "        # Sample data if needed\n",
    "        if len(self.X) > sample_size:\n",
    "            X_sample = self.X.sample(sample_size, random_state=42)\n",
    "        else:\n",
    "            X_sample = self.X\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        self.explainer = shap.TreeExplainer(self.model.named_estimators_['rf'])\n",
    "        self.shap_values = self.explainer.shap_values(X_sample)\n",
    "        \n",
    "        print(\"âœ… SHAP analysis complete\")\n",
    "        \n",
    "    def plot_feature_impact(self) -> None:\n",
    "        \"\"\"\n",
    "        Create SHAP summary plot of feature impacts.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(\n",
    "            self.shap_values[1], \n",
    "            self.X, \n",
    "            plot_size=(12, 8),\n",
    "            show=False\n",
    "        )\n",
    "        plt.title('Feature Impact Analysis (SHAP)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def analyze_prediction_confidence(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze prediction confidence distribution.\n",
    "        \"\"\"\n",
    "        # Get prediction probabilities\n",
    "        y_prob = self.model.predict_proba(self.X)[:, 1]\n",
    "        \n",
    "        # Plot confidence distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(y_prob, bins=50)\n",
    "        plt.title('Prediction Confidence Distribution')\n",
    "        plt.xlabel('Predicted Probability of No-Show')\n",
    "        plt.ylabel('Count')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze confidence levels\n",
    "        confidence_levels = pd.cut(\n",
    "            y_prob,\n",
    "            bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "        ).value_counts().sort_index()\n",
    "        \n",
    "        print(\"\\nðŸ“Š Prediction Confidence Levels:\")\n",
    "        for level, count in confidence_levels.items():\n",
    "            print(f\"   â€¢ {level}: {count:,} predictions ({count/len(y_prob):.1%})\")\n",
    "            \n",
    "    def analyze_errors(self) -> None:\n",
    "        \"\"\"\n",
    "        Analyze prediction errors and edge cases.\n",
    "        \"\"\"\n",
    "        # Get predictions\n",
    "        y_pred = self.model.predict(self.X)\n",
    "        y_prob = self.model.predict_proba(self.X)[:, 1]\n",
    "        \n",
    "        # Create error analysis DataFrame\n",
    "        error_analysis = pd.DataFrame({\n",
    "            'actual': self.y,\n",
    "            'predicted': y_pred,\n",
    "            'probability': y_prob,\n",
    "            'error': y_pred != self.y\n",
    "        })\n",
    "        \n",
    "        # Analyze high-confidence errors\n",
    "        high_conf_errors = error_analysis[\n",
    "            (error_analysis['error']) & \n",
    "            ((error_analysis['probability'] > 0.8) | (error_analysis['probability'] < 0.2))\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nðŸ” Error Analysis:\")\n",
    "        print(f\"   â€¢ Total errors: {error_analysis['error'].sum():,} ({error_analysis['error'].mean():.1%})\")\n",
    "        print(f\"   â€¢ High confidence errors: {len(high_conf_errors):,} ({len(high_conf_errors)/len(error_analysis):.1%})\")\n",
    "        \n",
    "        # Plot error distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x='actual', y='probability', data=error_analysis)\n",
    "        plt.title('Prediction Probability Distribution by Actual Class')\n",
    "        plt.xlabel('Actual No-Show')\n",
    "        plt.ylabel('Predicted Probability')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize interpretation\n",
    "interpreter = ModelInterpretation(model_dev.final_model, X_test, y_test)\n",
    "\n",
    "# Generate and plot SHAP values\n",
    "interpreter.generate_shap_values()\n",
    "interpreter.plot_feature_impact()\n",
    "\n",
    "# Analyze predictions\n",
    "interpreter.analyze_prediction_confidence()\n",
    "interpreter.analyze_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e67f886",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation Summary\n",
    "\n",
    "### Key Findings:\n",
    "1. **Model Performance**\n",
    "   - Achieved robust ROC-AUC scores through ensemble stacking\n",
    "   - Successfully handled class imbalance\n",
    "   - Identified optimal decision thresholds\n",
    "\n",
    "2. **Feature Insights**\n",
    "   - Discovered key predictors of no-shows\n",
    "   - Quantified feature interactions through SHAP analysis\n",
    "   - Validated feature engineering decisions\n",
    "\n",
    "3. **Operational Implications**\n",
    "   - Identified high-confidence prediction ranges\n",
    "   - Analyzed error patterns for potential improvements\n",
    "   - Determined optimal confidence thresholds for interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07d5a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
