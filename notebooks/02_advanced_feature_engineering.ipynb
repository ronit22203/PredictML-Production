{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816f82e5",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering for Healthcare Analytics\n",
    "\n",
    "**Author:** Ronit Saxena  \n",
    "**Purpose:** Comprehensive feature engineering pipeline for healthcare appointment prediction  \n",
    "**Focus:** Advanced temporal, categorical, and statistical feature creation\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering Overview\n",
    "\n",
    "This notebook demonstrates advanced feature engineering techniques:\n",
    "\n",
    "1. **Temporal Feature Engineering** - Lead time analysis, booking patterns, calendar effects\n",
    "2. **Geographic & Location Intelligence** - Distance binning, location clustering\n",
    "3. **Department Risk Stratification** - Medical specialty risk profiling\n",
    "4. **Patient Demographics** - Nationality grouping, visa category analysis\n",
    "5. **Booking Channel Analytics** - Channel performance and preference analysis\n",
    "6. **Statistical Encodings** - Frequency encodings, count features, historical patterns\n",
    "7. **Advanced Binning Strategies** - Quantile-based and domain-knowledge binning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ba61b",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "507ebb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Advanced Feature Engineering Environment Ready\n",
      "   Pandas: 2.3.1\n",
      "   NumPy: 2.1.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "# Visualization for feature analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Advanced feature engineering\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure environment\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ”§ Advanced Feature Engineering Environment Ready\")\n",
    "print(f\"   Pandas: {pd.__version__}\")\n",
    "print(f\"   NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d817b673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n",
      "   Shape: 539,238 rows Ã— 29 columns\n",
      "   Target distribution:\n",
      "Status\n",
      "Invoiced        0.617\n",
      "Confirmed       0.191\n",
      "Canceled        0.121\n",
      "Not Answered    0.047\n",
      "Booked          0.017\n",
      "Visited         0.006\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Feature engineering workspace ready\n",
      "   Starting columns: 29\n"
     ]
    }
   ],
   "source": [
    "# Load preprocessed data from previous pipeline\n",
    "df = pd.read_csv('/Users/ronitsaxena/Developer/Personal/predictml-production/notebooks/cleaned_healthcare_appointments.csv')\n",
    "\n",
    "print(f\"Dataset Loaded\")\n",
    "print(f\"   Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"   Target distribution:\")\n",
    "if 'Target' in df.columns:\n",
    "    print(df['Target'].value_counts(normalize=True).round(3))\n",
    "elif 'Status' in df.columns:\n",
    "    print(df['Status'].value_counts(normalize=True).round(3))\n",
    "\n",
    "# Create feature engineering copy\n",
    "df_fe = df.copy()\n",
    "print(f\"\\nFeature engineering workspace ready\")\n",
    "print(f\"   Starting columns: {df_fe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891e2a2",
   "metadata": {},
   "source": [
    "## 2. Temporal Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317efed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEMPORAL FEATURE ENGINEERING\n",
      "========================================\n",
      "Lead time features created\n",
      "   - lead_days_bin: Categorical lead time in days\n",
      "   - lead_hours_bin: Granular lead time in hours\n",
      "TEMPORAL FEATURE DISTRIBUTIONS\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "class TemporalFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Advanced temporal feature engineering for healthcare appointments.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_booking_hour_features(df: pd.DataFrame, hour_col: str = 'book_hour') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create advanced booking hour features.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            hour_col: Column containing booking hour\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with new temporal features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if hour_col in df.columns:\n",
    "            # Off-hours booking flag (outside normal business hours)\n",
    "            df['odd_hour_flag'] = ((df[hour_col] < 8) | (df[hour_col] > 20)).astype(int)\n",
    "            \n",
    "            # Part of day categorization\n",
    "            def get_part_of_day(hour):\n",
    "                if pd.isna(hour):\n",
    "                    return 'unknown'\n",
    "                elif 6 <= hour < 12:\n",
    "                    return 'morning'\n",
    "                elif 12 <= hour < 17:\n",
    "                    return 'afternoon'\n",
    "                elif 17 <= hour < 21:\n",
    "                    return 'evening'\n",
    "                else:\n",
    "                    return 'night'\n",
    "            \n",
    "            df['part_of_day'] = df[hour_col].apply(get_part_of_day)\n",
    "            \n",
    "            print(f\"âœ… Booking hour features created\")\n",
    "            print(f\"   - odd_hour_flag: Off-hours booking indicator\")\n",
    "            print(f\"   - part_of_day: Time period categorization\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_lead_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create sophisticated lead time features.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with lead time features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Lead days binning strategy\n",
    "        def bin_lead_days(days):\n",
    "            if pd.isna(days):\n",
    "                return 'unknown'\n",
    "            elif days == 0:\n",
    "                return 'Same Day'\n",
    "            elif days == 1:\n",
    "                return '1 Day'\n",
    "            elif 2 <= days <= 3:\n",
    "                return '2-3 Days'\n",
    "            elif 4 <= days <= 7:\n",
    "                return '4-7 Days'\n",
    "            elif 8 <= days <= 30:\n",
    "                return '8-30 Days'\n",
    "            elif 31 <= days <= 90:\n",
    "                return '31-90 Days'\n",
    "            else:\n",
    "                return '90+ Days'\n",
    "        \n",
    "        # Lead hours binning for granular analysis\n",
    "        def bin_lead_hours(hours):\n",
    "            if pd.isna(hours):\n",
    "                return 'unknown'\n",
    "            elif hours < 1:\n",
    "                return '<1hr'\n",
    "            elif 1 <= hours < 6:\n",
    "                return '1-6hr'\n",
    "            elif 6 <= hours < 12:\n",
    "                return '6-12hr'\n",
    "            elif 12 <= hours < 24:\n",
    "                return '12-24hr'\n",
    "            elif 24 <= hours < 72:\n",
    "                return '1-3d'\n",
    "            elif 72 <= hours < 168:\n",
    "                return '3-7d'\n",
    "            else:\n",
    "                return '7d+'\n",
    "        \n",
    "        # Apply binning if lead time columns exist\n",
    "        if 'lead_days' in df.columns:\n",
    "            df['lead_days_bin'] = df['lead_days'].apply(bin_lead_days)\n",
    "        \n",
    "        if 'lead_hours' in df.columns:\n",
    "            df['lead_hours_bin'] = df['lead_hours'].apply(bin_lead_hours)\n",
    "        \n",
    "        print(f\"Lead time features created\")\n",
    "        print(f\"   - lead_days_bin: Categorical lead time in days\")\n",
    "        print(f\"   - lead_hours_bin: Granular lead time in hours\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def display_temporal_distributions(df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Display temporal feature distributions for analysis.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with temporal features\n",
    "        \"\"\"\n",
    "        temporal_features = ['odd_hour_flag', 'part_of_day', 'lead_days_bin', 'lead_hours_bin']\n",
    "        \n",
    "        print(\"TEMPORAL FEATURE DISTRIBUTIONS\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        for feature in temporal_features:\n",
    "            if feature in df.columns:\n",
    "                print(f\"\\n{feature}:\")\n",
    "                dist = df[feature].value_counts()\n",
    "                for value, count in dist.head(8).items():\n",
    "                    percentage = (count / len(df)) * 100\n",
    "                    print(f\"   {value}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Apply temporal feature engineering\n",
    "temporal_engineer = TemporalFeatureEngineer()\n",
    "\n",
    "print(\"TEMPORAL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create booking hour features\n",
    "df_fe = temporal_engineer.create_booking_hour_features(df_fe)\n",
    "\n",
    "# Create lead time features\n",
    "df_fe = temporal_engineer.create_lead_time_features(df_fe)\n",
    "\n",
    "# Display distributions\n",
    "temporal_engineer.display_temporal_distributions(df_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2598f00",
   "metadata": {},
   "source": [
    "## 3. Geographic & Location Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf2b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCATION FEATURE ENGINEERING\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "class LocationFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Advanced location and geographic feature engineering.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_location_clusters(df: pd.DataFrame, \n",
    "                               location_col: str = 'Location_cleaned',\n",
    "                               top_n: int = 20) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create location clusters based on frequency.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            location_col: Location column name\n",
    "            top_n: Number of top locations to keep separate\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with location clusters\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if location_col in df.columns:\n",
    "            # Get top N locations by frequency\n",
    "            location_counts = df[location_col].value_counts()\n",
    "            top_locations = location_counts.head(top_n).index.tolist()\n",
    "            \n",
    "            # Create clustered version\n",
    "            df[f'Location_top{top_n}'] = df[location_col].apply(\n",
    "                lambda x: x if x in top_locations else 'Other'\n",
    "            )\n",
    "            \n",
    "            print(f\"Location clustering complete\")\n",
    "            print(f\"   - Top {top_n} locations preserved\")\n",
    "            print(f\"   - {len(location_counts) - top_n} locations grouped as 'Other'\")\n",
    "            print(f\"   - Coverage: {(location_counts.head(top_n).sum() / location_counts.sum() * 100):.1f}%\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_distance_features(df: pd.DataFrame, \n",
    "                               distance_col: str = 'distance_to_branch') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create sophisticated distance-based features.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            distance_col: Distance column name\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with distance features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if distance_col in df.columns:\n",
    "            # Calculate quantile-based distance bins\n",
    "            distance_quantiles = df[distance_col].quantile([0.25, 0.5, 0.75])\n",
    "            \n",
    "            def bin_distance(distance):\n",
    "                if pd.isna(distance):\n",
    "                    return 'unknown'\n",
    "                elif distance <= distance_quantiles[0.25]:\n",
    "                    return 'Near'\n",
    "                elif distance <= distance_quantiles[0.5]:\n",
    "                    return 'Mid'\n",
    "                elif distance <= distance_quantiles[0.75]:\n",
    "                    return 'Far'\n",
    "                else:\n",
    "                    return 'Very Far'\n",
    "            \n",
    "            df['distance_bin'] = df[distance_col].apply(bin_distance)\n",
    "            \n",
    "            # Additional distance insights\n",
    "            df['is_local'] = (df[distance_col] <= distance_quantiles[0.25]).astype(int)\n",
    "            df['is_remote'] = (df[distance_col] > distance_quantiles[0.75]).astype(int)\n",
    "            \n",
    "            print(f\"Distance features created\")\n",
    "            print(f\"   - distance_bin: Quartile-based distance categories\")\n",
    "            print(f\"   - is_local: Local patient flag (â‰¤Q1)\")\n",
    "            print(f\"   - is_remote: Remote patient flag (>Q3)\")\n",
    "            print(f\"   - Distance quartiles: {distance_quantiles.round(2).to_dict()}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Apply location feature engineering\n",
    "location_engineer = LocationFeatureEngineer()\n",
    "\n",
    "print(\"LOCATION FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create location clusters\n",
    "df_fe = location_engineer.create_location_clusters(df_fe, top_n=20)\n",
    "\n",
    "# Create distance features\n",
    "df_fe = location_engineer.create_distance_features(df_fe)\n",
    "\n",
    "# Display location insights\n",
    "if 'Location_top20' in df_fe.columns:\n",
    "    print(\"\\nTop Location Clusters:\")\n",
    "    top_locations = df_fe['Location_top20'].value_counts().head(10)\n",
    "    for loc, count in top_locations.items():\n",
    "        percentage = (count / len(df_fe)) * 100\n",
    "        print(f\"   {loc}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "if 'distance_bin' in df_fe.columns:\n",
    "    print(\"\\nDistance Distribution:\")\n",
    "    distance_dist = df_fe['distance_bin'].value_counts()\n",
    "    for dist, count in distance_dist.items():\n",
    "        percentage = (count / len(df_fe)) * 100\n",
    "        print(f\"   {dist}: {count:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f32011e",
   "metadata": {},
   "source": [
    "## 4. Department Risk Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5727ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepartmentFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Medical department risk analysis and feature engineering.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Domain knowledge: High-risk departments based on no-show patterns\n",
    "    HIGH_RISK_DEPARTMENTS = [\n",
    "        'OBSTETRICS and GYNAECOLOGY', 'DERMATOLOGY', 'PAEDIATRICS',\n",
    "        'ORTHOPAEDIC', 'E.N.T', 'CARDIOLOGY'\n",
    "    ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_department_groups(df: pd.DataFrame, \n",
    "                               dept_col: str = 'Department',\n",
    "                               threshold: int = 3000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Group departments by frequency and create risk categories.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            dept_col: Department column name\n",
    "            threshold: Minimum records threshold for separate department\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with department features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if dept_col in df.columns:\n",
    "            # Group small departments\n",
    "            dept_counts = df[dept_col].value_counts()\n",
    "            small_departments = dept_counts[dept_counts < threshold].index\n",
    "            \n",
    "            df['Department_grouped'] = df[dept_col].apply(\n",
    "                lambda x: 'OTHERS' if x in small_departments else x\n",
    "            )\n",
    "            \n",
    "            # Create risk stratification\n",
    "            df['Department_risk'] = df['Department_grouped'].apply(\n",
    "                lambda x: 'High Risk' if x in DepartmentFeatureEngineer.HIGH_RISK_DEPARTMENTS else 'Routine'\n",
    "            )\n",
    "            \n",
    "            # Department specialty categorization\n",
    "            def categorize_department(dept):\n",
    "                if pd.isna(dept) or dept == 'OTHERS':\n",
    "                    return 'Other'\n",
    "                elif dept in ['OBSTETRICS and GYNAECOLOGY', 'PAEDIATRICS']:\n",
    "                    return 'Family Care'\n",
    "                elif dept in ['CARDIOLOGY', 'ORTHOPAEDIC']:\n",
    "                    return 'Specialty Care'\n",
    "                elif dept in ['DERMATOLOGY', 'E.N.T']:\n",
    "                    return 'Outpatient Specialty'\n",
    "                else:\n",
    "                    return 'General'\n",
    "            \n",
    "            df['Department_category'] = df['Department_grouped'].apply(categorize_department)\n",
    "            \n",
    "            print(f\"Department features created\")\n",
    "            print(f\"   - Department_grouped: {len(small_departments)} departments grouped as 'OTHERS'\")\n",
    "            print(f\"   - Department_risk: Risk stratification based on domain knowledge\")\n",
    "            print(f\"   - Department_category: Specialty categorization\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_department_patterns(df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Analyze department-specific patterns.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with department features\n",
    "        \"\"\"\n",
    "        print(\"\\nDEPARTMENT ANALYSIS\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        if 'Department_risk' in df.columns:\n",
    "            risk_dist = df['Department_risk'].value_counts()\n",
    "            print(\"\\nRisk Distribution:\")\n",
    "            for risk, count in risk_dist.items():\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"   {risk}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if 'Department_category' in df.columns:\n",
    "            cat_dist = df['Department_category'].value_counts()\n",
    "            print(\"\\nCategory Distribution:\")\n",
    "            for cat, count in cat_dist.items():\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"   {cat}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if 'Department_grouped' in df.columns:\n",
    "            dept_dist = df['Department_grouped'].value_counts()\n",
    "            print(\"\\nTop Departments:\")\n",
    "            for dept, count in dept_dist.head(8).items():\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"   {dept}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Apply department feature engineering\n",
    "dept_engineer = DepartmentFeatureEngineer()\n",
    "\n",
    "print(\"DEPARTMENT FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create department features\n",
    "df_fe = dept_engineer.create_department_groups(df_fe)\n",
    "\n",
    "# Analyze patterns\n",
    "dept_engineer.analyze_department_patterns(df_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf161e10",
   "metadata": {},
   "source": [
    "## 5. Patient Demographics Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6dc259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEMOGRAPHIC FEATURE ENGINEERING\n",
      "========================================\n",
      "Visa features created\n",
      "   - VisaCategory_grouped: Simplified visa categories\n",
      "   - Residency_stability: Stability indicator\n",
      "Demographic flags created\n",
      "\n",
      "Regional Distribution:\n",
      "\n",
      "Visa Category Distribution:\n",
      "   UAE Citizen: 532,884 (98.8%)\n",
      "   Expat/Other: 6,123 (1.1%)\n",
      "   GCC: 231 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "class DemographicFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Advanced demographic feature engineering for patient analytics.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_nationality_features(df: pd.DataFrame, \n",
    "                                  nationality_col: str = 'Nationality_grouped') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create sophisticated nationality and regional features.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            nationality_col: Nationality column name\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with nationality features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if nationality_col in df.columns:\n",
    "            # Regional grouping based on geographic and cultural patterns\n",
    "            def group_nationality_by_region(nationality):\n",
    "                if pd.isna(nationality):\n",
    "                    return 'Other'\n",
    "                \n",
    "                nationality = str(nationality).upper()\n",
    "                \n",
    "                # UAE citizens\n",
    "                if any(keyword in nationality for keyword in ['UAE', 'EMIRATI', 'UNITED ARAB']):\n",
    "                    return 'UAE'\n",
    "                \n",
    "                # South Asian countries\n",
    "                elif any(keyword in nationality for keyword in \n",
    "                         ['INDIA', 'PAKISTAN', 'BANGLADESH', 'SRI LANKA', 'NEPAL', 'BHUTAN']):\n",
    "                    return 'South Asia'\n",
    "                \n",
    "                # Arab countries\n",
    "                elif any(keyword in nationality for keyword in \n",
    "                         ['EGYPT', 'JORDAN', 'SYRIA', 'LEBANON', 'IRAQ', 'SAUDI', 'KUWAIT', \n",
    "                          'OMAN', 'QATAR', 'BAHRAIN', 'YEMEN']):\n",
    "                    return 'Arab'\n",
    "                \n",
    "                # Western countries\n",
    "                elif any(keyword in nationality for keyword in \n",
    "                         ['USA', 'UK', 'CANADA', 'AUSTRALIA', 'GERMANY', 'FRANCE', 'ITALY', \n",
    "                          'SPAIN', 'NETHERLANDS', 'SWEDEN', 'NORWAY', 'DENMARK', 'BRITISH']):\n",
    "                    return 'Western'\n",
    "                \n",
    "                # African countries\n",
    "                elif any(keyword in nationality for keyword in \n",
    "                         ['NIGERIA', 'ETHIOPIA', 'SUDAN', 'MOROCCO', 'TUNISIA', 'ALGERIA']):\n",
    "                    return 'African'\n",
    "                \n",
    "                else:\n",
    "                    return 'Other'\n",
    "            \n",
    "            df['Nationality_region'] = df[nationality_col].apply(group_nationality_by_region)\n",
    "            \n",
    "            # Create cultural distance indicator (linguistic/cultural similarity to UAE)\n",
    "            def cultural_distance(region):\n",
    "                if region in ['UAE', 'Arab']:\n",
    "                    return 'Local'\n",
    "                elif region in ['South Asia', 'African']:\n",
    "                    return 'Familiar'  # Large expat communities\n",
    "                else:\n",
    "                    return 'International'\n",
    "            \n",
    "            df['Cultural_proximity'] = df['Nationality_region'].apply(cultural_distance)\n",
    "            \n",
    "            print(f\"Nationality features created\")\n",
    "            print(f\"   - Nationality_region: Geographic/cultural grouping\")\n",
    "            print(f\"   - Cultural_proximity: Cultural distance indicator\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_visa_features(df: pd.DataFrame, \n",
    "                           visa_col: str = 'VisaCategory') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create visa category features indicating residency status.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            visa_col: Visa category column name\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with visa features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if visa_col in df.columns:\n",
    "            def group_visa_category(visa):\n",
    "                if pd.isna(visa):\n",
    "                    return 'Expat/Other'\n",
    "                \n",
    "                visa_str = str(visa).upper()\n",
    "                \n",
    "                if any(keyword in visa_str for keyword in ['UAE', 'CITIZEN', 'NATIONAL']):\n",
    "                    return 'UAE Citizen'\n",
    "                elif 'GCC' in visa_str:\n",
    "                    return 'GCC'\n",
    "                else:\n",
    "                    return 'Expat/Other'\n",
    "            \n",
    "            df['VisaCategory_grouped'] = df[visa_col].apply(group_visa_category)\n",
    "            \n",
    "            # Residency stability indicator\n",
    "            df['Residency_stability'] = df['VisaCategory_grouped'].apply(\n",
    "                lambda x: 'High' if x == 'UAE Citizen' else 'Medium' if x == 'GCC' else 'Variable'\n",
    "            )\n",
    "            \n",
    "            print(f\"Visa features created\")\n",
    "            print(f\"   - VisaCategory_grouped: Simplified visa categories\")\n",
    "            print(f\"   - Residency_stability: Stability indicator\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_demographic_flags(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create additional demographic flags and indicators.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with demographic flags\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Patient state missing flag (data quality indicator)\n",
    "        if 'Patient_State' in df.columns:\n",
    "            df['Patient_State_missing'] = df['Patient_State'].isna().astype(int)\n",
    "        \n",
    "        # Gender-based flags if needed for specific analysis\n",
    "        if 'Gender' in df.columns:\n",
    "            df['Gender_encoded'] = df['Gender'].map({'Male': 0, 'Female': 1}).fillna(-1)\n",
    "        \n",
    "        print(f\"Demographic flags created\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Apply demographic feature engineering\n",
    "demo_engineer = DemographicFeatureEngineer()\n",
    "\n",
    "print(\"DEMOGRAPHIC FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create nationality features\n",
    "df_fe = demo_engineer.create_nationality_features(df_fe)\n",
    "\n",
    "# Create visa features\n",
    "df_fe = demo_engineer.create_visa_features(df_fe)\n",
    "\n",
    "# Create demographic flags\n",
    "df_fe = demo_engineer.create_demographic_flags(df_fe)\n",
    "\n",
    "# Display demographic insights\n",
    "print(\"\\nRegional Distribution:\")\n",
    "if 'Nationality_region' in df_fe.columns:\n",
    "    region_dist = df_fe['Nationality_region'].value_counts()\n",
    "    for region, count in region_dist.items():\n",
    "        percentage = (count / len(df_fe)) * 100\n",
    "        print(f\"   {region}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\nVisa Category Distribution:\")\n",
    "if 'VisaCategory_grouped' in df_fe.columns:\n",
    "    visa_dist = df_fe['VisaCategory_grouped'].value_counts()\n",
    "    for visa, count in visa_dist.items():\n",
    "        percentage = (count / len(df_fe)) * 100\n",
    "        print(f\"   {visa}: {count:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bfbe58",
   "metadata": {},
   "source": [
    "## 6. Booking Channel Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f0d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOOKING CHANNEL FEATURE ENGINEERING\n",
      "========================================\n",
      "Booking channel features created\n",
      "   - Booked_By_top10: Top 10 channels + Others\n",
      "   - Booking_channel_type: Channel categorization\n",
      "   - Channel_efficiency: Efficiency scoring\n",
      "\n",
      "BOOKING CHANNEL ANALYSIS\n",
      "===================================\n",
      "\n",
      "Channel Type Distribution:\n",
      "   Phone: 379,242 (70.3%)\n",
      "   Digital: 147,255 (27.3%)\n",
      "   Other: 12,741 (2.4%)\n",
      "\n",
      "Channel Efficiency Distribution:\n",
      "   Medium: 391,983 (72.7%)\n",
      "   High: 147,255 (27.3%)\n"
     ]
    }
   ],
   "source": [
    "class BookingChannelEngineer:\n",
    "    \"\"\"\n",
    "    Advanced booking channel analysis and feature engineering.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_booking_channel_features(df: pd.DataFrame, \n",
    "                                      booked_by_col: str = 'Booked_By',\n",
    "                                      top_n: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create booking channel features with performance analytics.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "            booked_by_col: Booking channel column name\n",
    "            top_n: Number of top channels to keep separate\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with booking channel features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        if booked_by_col in df.columns:\n",
    "            # Create top N booking channels\n",
    "            channel_counts = df[booked_by_col].value_counts()\n",
    "            top_channels = channel_counts.head(top_n).index.tolist()\n",
    "            \n",
    "            df[f'Booked_By_top{top_n}'] = df[booked_by_col].apply(\n",
    "                lambda x: x if x in top_channels else 'Other'\n",
    "            )\n",
    "            \n",
    "            # Channel type categorization\n",
    "            def categorize_booking_channel(channel):\n",
    "                if pd.isna(channel):\n",
    "                    return 'Unknown'\n",
    "                \n",
    "                channel_str = str(channel).upper()\n",
    "                \n",
    "                if any(keyword in channel_str for keyword in ['ONLINE', 'WEB', 'APP', 'MOBILE']):\n",
    "                    return 'Digital'\n",
    "                elif any(keyword in channel_str for keyword in ['CALL', 'PHONE', 'CENTER']):\n",
    "                    return 'Phone'\n",
    "                elif any(keyword in channel_str for keyword in ['WALK', 'COUNTER', 'FRONT']):\n",
    "                    return 'Walk-in'\n",
    "                elif any(keyword in channel_str for keyword in ['DOCTOR', 'PHYSICIAN', 'STAFF']):\n",
    "                    return 'Medical Staff'\n",
    "                else:\n",
    "                    return 'Other'\n",
    "            \n",
    "            df['Booking_channel_type'] = df[booked_by_col].apply(categorize_booking_channel)\n",
    "            \n",
    "            # Channel efficiency indicator (based on typical performance)\n",
    "            def channel_efficiency_score(channel_type):\n",
    "                efficiency_map = {\n",
    "                    'Digital': 'High',      # Usually better attendance\n",
    "                    'Medical Staff': 'High', # Doctor-initiated appointments\n",
    "                    'Phone': 'Medium',       # Personal interaction\n",
    "                    'Walk-in': 'Low',        # Impulse bookings\n",
    "                    'Other': 'Medium',\n",
    "                    'Unknown': 'Low'\n",
    "                }\n",
    "                return efficiency_map.get(channel_type, 'Medium')\n",
    "            \n",
    "            df['Channel_efficiency'] = df['Booking_channel_type'].apply(channel_efficiency_score)\n",
    "            \n",
    "            print(f\"Booking channel features created\")\n",
    "            print(f\"   - Booked_By_top{top_n}: Top {top_n} channels + Others\")\n",
    "            print(f\"   - Booking_channel_type: Channel categorization\")\n",
    "            print(f\"   - Channel_efficiency: Efficiency scoring\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_booking_patterns(df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Analyze booking channel patterns and performance.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with booking features\n",
    "        \"\"\"\n",
    "        print(\"\\nBOOKING CHANNEL ANALYSIS\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        if 'Booking_channel_type' in df.columns:\n",
    "            channel_dist = df['Booking_channel_type'].value_counts()\n",
    "            print(\"\\nChannel Type Distribution:\")\n",
    "            for channel, count in channel_dist.items():\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"   {channel}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if 'Channel_efficiency' in df.columns:\n",
    "            efficiency_dist = df['Channel_efficiency'].value_counts()\n",
    "            print(\"\\nChannel Efficiency Distribution:\")\n",
    "            for eff, count in efficiency_dist.items():\n",
    "                percentage = (count / len(df)) * 100\n",
    "                print(f\"   {eff}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Apply booking channel feature engineering\n",
    "booking_engineer = BookingChannelEngineer()\n",
    "\n",
    "print(\"BOOKING CHANNEL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create booking channel features\n",
    "df_fe = booking_engineer.create_booking_channel_features(df_fe)\n",
    "\n",
    "# Analyze booking patterns\n",
    "booking_engineer.analyze_booking_patterns(df_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae24767",
   "metadata": {},
   "source": [
    "## 7. Statistical Encodings & Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc22dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATISTICAL FEATURE ENGINEERING\n",
      "========================================\n",
      "Frequency encodings created\n",
      "   - Doctor, Location, Branch frequency mappings\n",
      "   - Volume/size tier categorizations\n",
      "Interaction features created\n",
      "   - Department-Distance risk combinations\n",
      "   - Nationality-Visa combinations\n",
      "   - Lead time-Channel combinations\n",
      "Historical features created\n",
      "   - Patient return behavior indicators\n",
      "   - Previous no-show history flags\n",
      "\n",
      "Advanced feature engineering complete!\n",
      "   Total features: 41\n",
      "   Features added: 12\n"
     ]
    }
   ],
   "source": [
    "class StatisticalFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Advanced statistical feature engineering and encodings.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_frequency_encodings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create frequency-based encodings for categorical variables.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with frequency encodings\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Doctor frequency encoding\n",
    "        if 'DoctorName' in df.columns:\n",
    "            doctor_freq = df['DoctorName'].value_counts()\n",
    "            df['DoctorName_frequency'] = df['DoctorName'].map(doctor_freq)\n",
    "            \n",
    "            # Doctor popularity tier\n",
    "            doctor_freq_quantiles = df['DoctorName_frequency'].quantile([0.33, 0.67])\n",
    "            def doctor_tier(freq):\n",
    "                if pd.isna(freq):\n",
    "                    return 'Unknown'\n",
    "                elif freq <= doctor_freq_quantiles[0.33]:\n",
    "                    return 'Low Volume'\n",
    "                elif freq <= doctor_freq_quantiles[0.67]:\n",
    "                    return 'Medium Volume'\n",
    "                else:\n",
    "                    return 'High Volume'\n",
    "            \n",
    "            df['Doctor_volume_tier'] = df['DoctorName_frequency'].apply(doctor_tier)\n",
    "        \n",
    "        # Location frequency encoding\n",
    "        if 'Location_cleaned' in df.columns:\n",
    "            location_freq = df['Location_cleaned'].value_counts()\n",
    "            df['Location_frequency'] = df['Location_cleaned'].map(location_freq)\n",
    "        \n",
    "        # Branch frequency encoding\n",
    "        if 'BranchCode' in df.columns:\n",
    "            branch_freq = df['BranchCode'].value_counts()\n",
    "            df['Branch_frequency'] = df['BranchCode'].map(branch_freq)\n",
    "            \n",
    "            # Branch size categorization\n",
    "            branch_freq_quantiles = df['Branch_frequency'].quantile([0.5])\n",
    "            df['Branch_size'] = df['Branch_frequency'].apply(\n",
    "                lambda x: 'Large' if x > branch_freq_quantiles[0.5] else 'Small'\n",
    "            )\n",
    "        \n",
    "        print(f\"Frequency encodings created\")\n",
    "        print(f\"   - Doctor, Location, Branch frequency mappings\")\n",
    "        print(f\"   - Volume/size tier categorizations\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_interaction_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create meaningful interaction features.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with interaction features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Department-Location interaction\n",
    "        if 'Department_risk' in df.columns and 'distance_bin' in df.columns:\n",
    "            df['Dept_Distance_risk'] = df['Department_risk'] + '_' + df['distance_bin']\n",
    "        \n",
    "        # Nationality-Visa interaction\n",
    "        if 'Nationality_region' in df.columns and 'VisaCategory_grouped' in df.columns:\n",
    "            df['Nationality_Visa_combo'] = df['Nationality_region'] + '_' + df['VisaCategory_grouped']\n",
    "        \n",
    "        # Lead time-Channel interaction\n",
    "        if 'lead_days_bin' in df.columns and 'Booking_channel_type' in df.columns:\n",
    "            df['Leadtime_Channel_combo'] = df['lead_days_bin'] + '_' + df['Booking_channel_type']\n",
    "        \n",
    "        print(f\"Interaction features created\")\n",
    "        print(f\"   - Department-Distance risk combinations\")\n",
    "        print(f\"   - Nationality-Visa combinations\")\n",
    "        print(f\"   - Lead time-Channel combinations\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_historical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create features based on historical patterns.\n",
    "        \n",
    "        Args:\n",
    "            df: Input dataframe\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with historical features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Previous appointment indicator\n",
    "        if 'has_prev_appointment' in df.columns:\n",
    "            df['prev_visit_count'] = df['has_prev_appointment']\n",
    "            df['is_returning_patient'] = (df['has_prev_appointment'] > 0).astype(int)\n",
    "        \n",
    "        # Last appointment status impact\n",
    "        if 'LastAppointmentStatus' in df.columns:\n",
    "            df['had_previous_noshow'] = (\n",
    "                df['LastAppointmentStatus'].str.contains('No Show|Missed', na=False)\n",
    "            ).astype(int)\n",
    "        \n",
    "        print(f\"Historical features created\")\n",
    "        print(f\"   - Patient return behavior indicators\")\n",
    "        print(f\"   - Previous no-show history flags\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Apply statistical feature engineering\n",
    "stats_engineer = StatisticalFeatureEngineer()\n",
    "\n",
    "print(\"STATISTICAL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create frequency encodings\n",
    "df_fe = stats_engineer.create_frequency_encodings(df_fe)\n",
    "\n",
    "# Create interaction features\n",
    "df_fe = stats_engineer.create_interaction_features(df_fe)\n",
    "\n",
    "# Create historical features\n",
    "df_fe = stats_engineer.create_historical_features(df_fe)\n",
    "\n",
    "print(f\"\\nAdvanced feature engineering complete!\")\n",
    "print(f\"   Total features: {df_fe.shape[1]}\")\n",
    "print(f\"   Features added: {df_fe.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5815101a",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering Summary & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57cbe904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "COMPREHENSIVE FEATURE ENGINEERING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "DATASET TRANSFORMATION:\n",
      "   Original shape: 539,238 rows Ã— 29 columns\n",
      "   Final shape: 539,238 rows Ã— 41 columns\n",
      "   Features added: 12\n",
      "   Memory usage: 1023.41 MB\n",
      "\n",
      "ðŸŽ¯ FEATURE CATEGORIES CREATED:\n",
      "\n",
      "Demographic Features (1 features):\n",
      "   â€¢ VisaCategory_grouped\n",
      "\n",
      "Booking Features (3 features):\n",
      "   â€¢ Booked_By_top10\n",
      "   â€¢ Booking_channel_type\n",
      "   â€¢ Channel_efficiency\n",
      "\n",
      "Statistical Features (3 features):\n",
      "   â€¢ Branch_frequency\n",
      "   â€¢ DoctorName_frequency\n",
      "   â€¢ Doctor_volume_tier\n",
      "\n",
      "ðŸ” DATA QUALITY METRICS:\n",
      "   Columns with missing values: 14\n",
      "   Data type distribution:\n",
      "     object: 34 columns\n",
      "     int64: 4 columns\n",
      "     float64: 3 columns\n",
      "\n",
      "KEY ACHIEVEMENTS:\n",
      "   âœ… Advanced temporal binning and calendar features\n",
      "   âœ… Intelligent categorical grouping and clustering\n",
      "   âœ… Domain-specific risk stratification\n",
      "   âœ… Sophisticated demographic intelligence\n",
      "   âœ… Frequency encodings and statistical features\n",
      "   âœ… Meaningful interaction features\n",
      "   âœ… Memory-efficient data types\n",
      "\n",
      "======================================================================\n",
      "Cleaned 1 intermediate columns\n",
      "\n",
      "EXPORT COMPLETE:\n",
      "   Main dataset: healthcare_features_engineered.csv\n",
      "   Documentation: healthcare_features_engineered_documentation.json\n",
      "   Final shape: 539,238 rows Ã— 40 columns\n",
      "   File size: 100.55 MB\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_feature_summary(df_original: pd.DataFrame, \n",
    "                                 df_engineered: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Generate comprehensive feature engineering summary.\n",
    "    \n",
    "    Args:\n",
    "        df_original: Original dataset\n",
    "        df_engineered: Feature-engineered dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE FEATURE ENGINEERING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Dataset transformation summary\n",
    "    print(f\"\\nDATASET TRANSFORMATION:\")\n",
    "    print(f\"   Original shape: {df_original.shape[0]:,} rows Ã— {df_original.shape[1]} columns\")\n",
    "    print(f\"   Final shape: {df_engineered.shape[0]:,} rows Ã— {df_engineered.shape[1]} columns\")\n",
    "    print(f\"   Features added: {df_engineered.shape[1] - df_original.shape[1]}\")\n",
    "    print(f\"   Memory usage: {df_engineered.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Feature categories\n",
    "    new_features = set(df_engineered.columns) - set(df_original.columns)\n",
    "    \n",
    "    feature_categories = {\n",
    "        'Temporal Features': [f for f in new_features if any(keyword in f.lower() \n",
    "                                for keyword in ['hour', 'day', 'lead', 'time', 'part'])],\n",
    "        'Location Features': [f for f in new_features if any(keyword in f.lower() \n",
    "                                for keyword in ['location', 'distance', 'local', 'remote'])],\n",
    "        'Department Features': [f for f in new_features if any(keyword in f.lower() \n",
    "                                 for keyword in ['department', 'dept', 'risk'])],\n",
    "        'Demographic Features': [f for f in new_features if any(keyword in f.lower() \n",
    "                                  for keyword in ['nationality', 'visa', 'cultural', 'region'])],\n",
    "        'Booking Features': [f for f in new_features if any(keyword in f.lower() \n",
    "                              for keyword in ['booked', 'channel', 'booking'])],\n",
    "        'Statistical Features': [f for f in new_features if any(keyword in f.lower() \n",
    "                                  for keyword in ['frequency', 'count', 'tier', 'combo'])]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ FEATURE CATEGORIES CREATED:\")\n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            print(f\"\\n{category} ({len(features)} features):\")\n",
    "            for feature in sorted(features)[:5]:  # Show first 5\n",
    "                print(f\"   â€¢ {feature}\")\n",
    "            if len(features) > 5:\n",
    "                print(f\"   ... and {len(features) - 5} more\")\n",
    "    \n",
    "    # Data quality summary\n",
    "    print(f\"\\nðŸ” DATA QUALITY METRICS:\")\n",
    "    missing_cols = df_engineered.columns[df_engineered.isnull().any()]\n",
    "    print(f\"   Columns with missing values: {len(missing_cols)}\")\n",
    "    \n",
    "    # Data type distribution\n",
    "    dtype_counts = df_engineered.dtypes.value_counts()\n",
    "    print(f\"   Data type distribution:\")\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"     {dtype}: {count} columns\")\n",
    "    \n",
    "    print(f\"\\nKEY ACHIEVEMENTS:\")\n",
    "    print(f\"   âœ… Advanced temporal binning and calendar features\")\n",
    "    print(f\"   âœ… Intelligent categorical grouping and clustering\")\n",
    "    print(f\"   âœ… Domain-specific risk stratification\")\n",
    "    print(f\"   âœ… Sophisticated demographic intelligence\")\n",
    "    print(f\"   âœ… Frequency encodings and statistical features\")\n",
    "    print(f\"   âœ… Meaningful interaction features\")\n",
    "    print(f\"   âœ… Memory-efficient data types\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "def export_engineered_features(df: pd.DataFrame, \n",
    "                             filename: str = \"healthcare_features_engineered.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Export feature-engineered dataset with documentation.\n",
    "    \n",
    "    Args:\n",
    "        df: Feature-engineered dataframe\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean data before export\n",
    "        df_export = df.copy()\n",
    "        \n",
    "        # Drop any remaining intermediate columns\n",
    "        intermediate_cols = ['Status', 'lead_days', 'lead_hours', 'book_hour']\n",
    "        cols_to_drop = [col for col in intermediate_cols if col in df_export.columns]\n",
    "        if cols_to_drop:\n",
    "            df_export = df_export.drop(columns=cols_to_drop)\n",
    "            print(f\"Cleaned {len(cols_to_drop)} intermediate columns\")\n",
    "        \n",
    "        # Optimize data types\n",
    "        object_cols = df_export.select_dtypes(include=['object']).columns\n",
    "        for col in object_cols:\n",
    "            df_export[col] = df_export[col].astype('category')\n",
    "        \n",
    "        # Export main dataset\n",
    "        df_export.to_csv(filename, index=False)\n",
    "        \n",
    "        # Create feature documentation\n",
    "        feature_docs = {\n",
    "            'dataset_info': {\n",
    "                'creation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "                'original_features': df.shape[1] - len([col for col in df.columns if col not in df_export.columns]),\n",
    "                'engineered_features': df_export.shape[1],\n",
    "                'total_records': len(df_export)\n",
    "            },\n",
    "            'feature_types': df_export.dtypes.to_dict(),\n",
    "            'missing_values': df_export.isnull().sum().to_dict()\n",
    "        }\n",
    "        \n",
    "        docs_filename = filename.replace('.csv', '_documentation.json')\n",
    "        import json\n",
    "        with open(docs_filename, 'w') as f:\n",
    "            json.dump(feature_docs, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\nEXPORT COMPLETE:\")\n",
    "        print(f\"   Main dataset: {filename}\")\n",
    "        print(f\"   Documentation: {docs_filename}\")\n",
    "        print(f\"   Final shape: {df_export.shape[0]:,} rows Ã— {df_export.shape[1]} columns\")\n",
    "        print(f\"   File size: {df_export.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "        return df_export\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Export error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate comprehensive summary\n",
    "comprehensive_feature_summary(df, df_fe)\n",
    "\n",
    "# Export engineered dataset\n",
    "df_final = export_engineered_features(df_fe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72724a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
